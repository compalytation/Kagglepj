{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# numpy\nimport numpy as np\n\n# pandas stuff\nimport pandas as pd\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# plotting stuff\nfrom pandas.plotting import lag_plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\ncolorMap = sns.light_palette(\"blue\", as_cmap=True)\n#plt.rcParams.update({'font.size': 12})\n\n\n# install dabl\n!pip install dabl > /dev/null\nimport dabl\n# install datatable\n!pip install datatable > /dev/null\nimport datatable as dt\n\n# misc\nimport missingno as msno\n\n# system\nimport warnings\nwarnings.filterwarnings('ignore')\n# for the image import\nimport os\nfrom IPython.display import Image\n# garbage collector to keep RAM in check\nimport gc  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n#利用datatable載入data\n\n#train_data_datatable = dt.fread('../input/jane-street-market-prediction/train.csv')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#再轉為pandas方式  載入時間縮短很多！！\n\n#train_data = train_data_datatable.to_pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_explore_old(df):\n    # 複製data\n    data = df.copy()\n    # 查看列名\n    print(data.columns)\n    # 查看data\n    print(data.head())\n    # 有空值\n    print(data.sum())\n    # 看平均數\n    print(data.mean())\n    print(data.describe())\n    # 查看空值\n    print(data.isnull().sum())\n    # 最多的有1734個NAN，接近20%\n    fig = plt.figure()\n    fig, axes = plt.subplots(4, 2, sharex = True)\n    for i in range(4):\n        for j in range(2):\n            pos = 2*i + j\n            if pos > 6:\n                break\n            axes[i][j].set_title(data.columns[pos])\n            axes[i][j].plot(data.iloc[:, pos])\n    plt.subplots_adjust(wspace = 0.2, hspace = 1)\n    plt.savefig(\"./output/targets_line.png\")\n    plt.close()\n    # 畫特徵\n    fig = plt.figure(figsize = (10, 80))\n    for i in range(130):\n        ax = fig.add_subplot(65, 2, i+1)\n        ax.set_title(data.columns[i+7])\n        plt.plot(data.iloc[:, i+7])\n    plt.subplots_adjust(wspace = 0.2, hspace = 1)\n    plt.savefig(\"./output/features_line.png\")\n    plt.close()\n    \n    fig = plt.figure()\n    sns.distplot(data.iloc[:, 1:8], hist = True, bins = 100, kde = True)\n    # data.iloc[:, 1:8].plot.hist(subplots = True, sharex = True, layout = (4, 2), bins = 50)\n    plt.savefig(\"./output/targets_hist.png\")\n    # 化特徵\n    fig = plt.figure()\n    sns.distplot(data.iloc[:, 8:-2], hist = True, bins = 100, kde = True)\n    # data.iloc[:, 8:-2].plot.hist(subplots = True, sharex = True, layout = (65, 2), figsize = (10, 80), bins = 50)\n    plt.savefig(\"./output/features_hist.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#顯示上面def要顯示的東西\n#data_explore_old(train_data)  #記憶體不足 不顯示","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train_data['resp']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_ylabel (\"Cumulative resp\", fontsize=18);\nbalance.plot(lw=3);\ndel balance\ngc.collect();  #resp累積值","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train_data['resp']).cumsum()\nresp_1= pd.Series(train_data['resp_1']).cumsum()\nresp_2= pd.Series(train_data['resp_2']).cumsum()\nresp_3= pd.Series(train_data['resp_3']).cumsum()\nresp_4= pd.Series(train_data['resp_4']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative resp and time horizons 1, 2, 3, and 4 (500 days)\", fontsize=18)\nbalance.plot(lw=3)\nresp_1.plot(lw=3)\nresp_2.plot(lw=3)\nresp_3.plot(lw=3)\nresp_4.plot(lw=3)\nplt.legend(loc=\"upper left\");\ndel resp_1\ndel resp_2\ndel resp_3\ndel resp_4\ngc.collect();'''   #resp 1~4 累積圖","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_zeros = (100/train_data.shape[0])*((train_data.weight.values == 0).sum())\nprint('Percentage of zero weights is: %i' % percent_zeros +\"%\")\n#17%的weights為0 處理時將0拿掉\nmin_weight = train_data['weight'].min()\nprint('The minimum weight is: %.2f' % min_weight)\n#下面的確認沒有-的weights\nmax_weight = train_data['weight'].max()\nprint('The maximum weight was: %.2f' % max_weight)\n#最大weight為167.27\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''plt.figure(figsize = (12,5))\nax = sns.distplot(train_data['weight'], \n             bins=1400, \n             kde_kws={\"clip\":(0.001,1.4)}, \n             hist_kws={\"range\":(0.001,1.4)},\n             color='darkcyan', \n             kde=False);\nvalues = np.array([rec.get_height() for rec in ax.patches])\nnorm = plt.Normalize(values.min(), values.max())\ncolors = plt.cm.jet(norm(values))\nfor rec, col in zip(ax.patches, colors):\n    rec.set_color(col)\nplt.xlabel(\"Histogram of non-zero weights\", size=14)\nplt.show();\ndel values\ngc.collect();  #weights分布圖0.05~0.2感覺最多''' #weughts分布圖","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''train_data['weight_resp']   = train_data['weight']*train_data['resp']\ntrain_data['weight_resp_1'] = train_data['weight']*train_data['resp_1']\ntrain_data['weight_resp_2'] = train_data['weight']*train_data['resp_2']\ntrain_data['weight_resp_3'] = train_data['weight']*train_data['resp_3']\ntrain_data['weight_resp_4'] = train_data['weight']*train_data['resp_4']\n\nfig, ax = plt.subplots(figsize=(15, 5))\nresp    = pd.Series(1+(train_data.groupby('date')['weight_resp'].mean())).cumprod()\nresp_1  = pd.Series(1+(train_data.groupby('date')['weight_resp_1'].mean())).cumprod()\nresp_2  = pd.Series(1+(train_data.groupby('date')['weight_resp_2'].mean())).cumprod()\nresp_3  = pd.Series(1+(train_data.groupby('date')['weight_resp_3'].mean())).cumprod()\nresp_4  = pd.Series(1+(train_data.groupby('date')['weight_resp_4'].mean())).cumprod()\nax.set_xlabel (\"Day\", fontsize=18)\nax.set_title (\"Cumulative daily return for resp and time horizons 1, 2, 3, and 4 (500 days)\", fontsize=18)\nresp.plot(lw=3, label='resp x weight')\nresp_1.plot(lw=3, label='resp_1 x weight')\nresp_2.plot(lw=3, label='resp_2 x weight')\nresp_3.plot(lw=3, label='resp_3 x weight')\nresp_4.plot(lw=3, label='resp_4 x weight')\n# day 85 marker\nax.axvline(x=85, linestyle='--', alpha=0.3, c='red', lw=1)\nax.axvspan(0, 85 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nplt.legend(loc=\"lower left\");  '''\n#weight * resp的累積值\n#紫色收益最多 投資越久收益越大 但風險越高","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature 有130個匿名特徵 其中feature_0為1與-1\ntrain_data['feature_0'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2,figsize=(20,10))\n\nax1.plot((pd.Series(train_data['feature_1']).cumsum()), lw=3, color='red')\nax1.set_title (\"Linear\", fontsize=22);\nax1.axvline(x=514052, linestyle='--', alpha=0.3, c='green', lw=2)\nax1.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax1.set_xlim(xmin=0)\nax1.set_ylabel (\"feature_1\", fontsize=18);\n\nax2.plot((pd.Series(train_data['feature_3']).cumsum()), lw=3, color='green')\nax2.set_title (\"Noisy\", fontsize=22);\nax2.axvline(x=514052, linestyle='--', alpha=0.3, c='red', lw=2)\nax2.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax2.set_xlim(xmin=0)\nax2.set_ylabel (\"feature_3\", fontsize=18);\n\nax3.plot((pd.Series(train_data['feature_55']).cumsum()), lw=3, color='darkorange')\nax3.set_title (\"Hybryd (Tag 21)\", fontsize=22);\nax3.set_xlabel (\"Trade\", fontsize=18)\nax3.axvline(x=514052, linestyle='--', alpha=0.3, c='green', lw=2)\nax3.axvspan(0, 514052 , color=sns.xkcd_rgb['grey'], alpha=0.1)\nax3.set_xlim(xmin=0)\nax3.set_ylabel (\"feature_55\", fontsize=18);\n\nax4.plot((pd.Series(train_data['feature_73']).cumsum()), lw=3, color='blue')\nax4.set_title (\"Negative\", fontsize=22)\nax4.set_xlabel (\"Trade\", fontsize=18)\nax4.set_ylabel (\"feature_73\", fontsize=18);\ngc.collect();'''\n\n#四種特徵類型 拿出代表feature1.3.55.73","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''sns.set_palette(\"bright\")\n\nfig, axes = plt.subplots(2,2,figsize=(8,8))\n\nsns.distplot(train_data[['feature_60']], hist=True, bins=200,  ax=axes[0,0])\nsns.distplot(train_data[['feature_61']], hist=True, bins=200,  ax=axes[0,0])\naxes[0,0].set_title (\"features 60 and 61\", fontsize=18)\naxes[0,0].legend(labels=['60', '61'])\n\nsns.distplot(train_data[['feature_62']], hist=True,  bins=200, ax=axes[0,1])\nsns.distplot(train_data[['feature_63']], hist=True,  bins=200, ax=axes[0,1])\naxes[0,1].set_title (\"features 62 and 63\", fontsize=18)\naxes[0,1].legend(labels=['62', '63'])\n\nsns.distplot(train_data[['feature_65']], hist=True,  bins=200, ax=axes[1,0])\nsns.distplot(train_data[['feature_66']], hist=True,  bins=200, ax=axes[1,0])\naxes[1,0].set_title (\"features 65 and 66\", fontsize=18)\naxes[1,0].legend(labels=['65', '66'])\n\n\nsns.distplot(train_data[['feature_67']], hist=True,  bins=200, ax=axes[1,1])\nsns.distplot(train_data[['feature_68']], hist=True,  bins=200, ax=axes[1,1])\naxes[1,1].set_title (\"features 67 and 68\", fontsize=18)\naxes[1,1].legend(labels=['67', '68'])\n\nplt.show();\ngc.collect();'''\n\n#可以看出60.61 and 62.63 and 65.66 and 67.68相關度很高 \n#後面特徵工程會嘗試使用方法 減少特徵","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#del train_data, train_data_datatable#","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 500)\n\n\n# Standard plotly imports\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\nimport cufflinks as cf\nimport plotly.figure_factory as ff\nimport os\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\n\n\nfrom sklearn import preprocessing\nimport xgboost as xgb'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 500)\n\nimport cufflinks\nimport cufflinks as cf\n\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\n\n\nfrom sklearn import preprocessing\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\n#%%time\n#train = pd.read_csv('/kaggle/input/jane-street-market-prediction/train.csv')\n#載入太久\n\n# install dabl\n#!pip install dabl > /dev/null\n#import dabl\n# install datatable\n#!pip install datatable > /dev/null\n#import datatable as dt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#利用fread載入data\n\ntrain_data_datatable = dt.fread('../input/jane-street-market-prediction/train.csv')\n\n#再轉為pandas方式  載入時間縮短很多！！ 直接載入pandas要2分多\n\ntrain_data = train_data_datatable.to_pandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import janestreet\nenv = janestreet.make_env()\niter_test = env.iter_test()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_data[train_data['weight'] != 0]\ntrain['action'] = ((train['weight'].values * train['resp'].values) > 0).astype('int')#true為1 false為0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.loc[:, train.columns.str.contains('feature')] #取出包含feature標籤0~129\n#print(X_train)\ny_train = train.loc[:, 'action']\n#print(y_train)\n#y_train.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.fillna(-999)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import feature_selection as fs\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampler = TPESampler(seed=666)\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 12)\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 600)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0001, 0.99)\n    subsample = trial.suggest_uniform('subsample', 0.0001, 1.0)\n    colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.0000001, 1)\n    \n    model = XGBClassifier(\n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        learning_rate=learning_rate,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        random_state=666,\n        tree_method='gpu_hist'\n    )\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X_train, y_train)\n    score = accuracy_score(\n        y_train, \n        model.predict(X_train)\n    )\n    return score\n\n# study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n# study.optimize(objective, n_trials=70)\n# params = study.best_params\n# params['random_state'] = 666\n# params['tree_method'] = 'gpu_hist'\n\nparams1 = {\n    'max_depth': 8, \n    'n_estimators': 500, \n    'learning_rate': 0.01, \n    'subsample': 0.9, \n    'tree_method': 'gpu_hist',\n    'random_state': 666\n}\n\n# params2 = {\n#     'max_depth': 9, \n#     'n_estimators': 500, \n#     'learning_rate': 0.03, \n#     'subsample': 0.9, \n#     'tree_method': 'gpu_hist',\n#     'random_state': 666\n# }\n\nparams3 = {\n    'max_depth': 10, \n    'n_estimators': 500, \n    'learning_rate': 0.03, \n    'subsample': 0.9, \n    'colsample_bytree': 0.7,\n    'tree_method': 'gpu_hist',\n    'random_state': 666\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = XGBClassifier(**params1)\nmodel1.fit(X_train, y_train)\n\n# model2 = XGBClassifier(**params2)\n# model2.fit(X_train, y_train)\n\nmodel3 = XGBClassifier(**params3)\nmodel3.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for (test_df, sample_prediction_df) in iter_test:\n    if test_df['weight'].item() > 0:\n        X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n        X_test = X_test.fillna(-999)\n        # y_preds = model1.predict(X_test) + model2.predict(X_test) + model3.predict(X_test)\n        y_preds = model1.predict(X_test) + model3.predict(X_test)\n        if y_preds == 2:\n            y_preds = np.array([1])\n        else:\n            y_preds = np.array([0])\n    else:\n        y_preds = np.array([0])\n    sample_prediction_df.action = y_preds\n    env.predict(sample_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''min_features_to_select = 1  # Minimum number of features to consider\nrfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n              scoring='accuracy',\n              min_features_to_select=min_features_to_select)\nrfecv.fit(X_train, y_train)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#特徵  上面提到60.61 and 62.63 and 65.66 and 67.68 相關度很高 在這裡處理\n#weight 為0先拿掉\n'''print('data Len:',len(train_data))\ntrain_data = train_data[train_data['weight'] != 0]\nprint('data Len-weight=0:',len(train_data))\ntrain_data['action'] = ((train_data['weight'].values * train_data['resp'].values) > 0).astype('int')\n#print(train_data['action'])\n#weeight*resp 新增欄位設為要的答案\nX_train = train_data.loc[:, train_data.columns.str.contains('feature')]\n#print(X_train)\ny_train = train_data.loc[:, 'action']\n#print(y_train)\n#get X,y\nX_train = X_train.fillna(-999) #缺失值填補-999'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''from sklearn.decomposition import PCA  #PCA\nfrom sklearn import datasets\nimport seaborn as sns\n# 執行PCA之前，我們需要對特徵進行歸一化，以使其均值和單位方差為零\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_norm = scaler.transform(X_train)\n\npca = PCA()\ncomp = pca.fit(X_train_norm)\n'''\n#繪製圖表以顯示129個特徵中解釋的變化如何隨主要成分的數量而變化\n#plt.plot(np.cumsum(comp.explained_variance_ratio_))\n#plt.grid()\n#plt.xlabel('Number of Principal Components')\n#plt.ylabel('Explained Variance')\n#sns.despine();\n\n#前15個主要成分解釋了大約80％的變化\n#前40個主要成分解釋了大約95％的變化\n#pca = PCA(n_components=50).fit(X_train_norm)     #維度出使用50個features\n#X_train = pca.transform(X_train_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#del pca, comp, X_train_norm, scaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''clf = xgb.XGBClassifier(\n    n_estimators=400,\n    max_depth=7,\n    eta=0.5, # learning_rate\n    missing=None,\n    random_state=42,\n    tree_method='gpu_hist',\n    subsample=0.8,\n    colsample_bytree=1,\n    #sampling_method='gradient_based',\n    #eval_metric='logloss',\n    verbosity=2   # info\n)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%time\n#clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#clf.save_model('test.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#iter_test = X_teest 測試用","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''def fillna_npwhere(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''rcount = 0\nfor (test_df, prediction_df) in env.iter_test():\n    X_test = test_df.loc[:, features]\n    y_preds = clf.predict(X_test)\n    prediction_df.action = y_preds\n    env.predict(prediction_df)\n    rcount += len(test_df.index)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''for (test_df, sample_prediction_df) in iter_test:\n    X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n    X_test = X_test.fillna(-999)\n    \n    scaler = StandardScaler()\n    scaler.fit(X_test)\n    X_test_norm = scaler.transform(X_test)\n\n    #comp = pca.fit(X_test_norm)\n    #pca = PCA(n_components=50).fit(X_test_norm)     #維度出使用50個features\n    X_train = pca.transform(X_test_norm)\n    \n    \n    \n    y_preds = clf.predict(X_test)\n    sample_prediction_df.action = y_preds\n    submission=env.predict(sample_prediction_df)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''#for (test_df, sample_prediction_df) in iter_test:\nX_test = test_df.loc[:, test_df.columns.str.contains('feature')]\nX_test = X_test.fillna(-999)\nprint(X_test)\nX_test = Feature_PCA(X_test)\ny_preds = clf.predict(X_test)  #測試使用 (原本使用defPCA發現不行這樣用)\nsample_prediction_df.action = y_preds\nsubmission=env.predict(sample_prediction_df)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''X_test = X_test.loc[:, X_test.columns.str.contains('feature')]\nX_test = X_test.fillna(-999)\n\nX_test = Feature_PCA(X_test)\n\ny_preds = clf.predict(X_test)\n\nsample_prediction_df.action = y_preds\nenv.predict(sample_prediction_df)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission=pd.read_csv('../input/jane-street-market-prediction/example_sample_submission.csv',sep=',')\n\nsubmission.to_csv('./submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}